import pandas as pd
import numpy as np
import re
import time
from nltk.corpus import stopwords, wordnet
from string import punctuation
from nltk.stem import WordNetLemmatizer
from nltk.tag import pos_tag
from nltk.tokenize import word_tokenize


# # Preprocessing
sw = stopwords.words("english")
lemmatizer = WordNetLemmatizer()

# Defining a function that will clean, tokenize and lemmatize the data:


def preprocess(series):
    
    """ This function takes as argument a pd Series of email texts and performs the following operations:
    - cleans the texts
    - pos tags
    - lemmatizes"""
    
    start = time.time()
    
    def clean_df(col):
    
        """ This function takes as input a df column of dtype string, removes stopwords, punctuation, 
        numeric values and emails, filters out words of len(1) and returns the text tokens"""
    
        col = word_tokenize(col.lower())
        col = [word for word in col if word not in sw]
        col = [word for word in col if word not in punctuation]
        col = [word.strip(punctuation) for word in col]
        col = " ".join([word for word in col if not word.isdigit()])
        col = re.sub(r'http\S*|www\S*', '', col)
        col = re.findall(r'[a-z]+', col)
        col = [word for word in col if len(word) > 1]
        return col
    
    # Applying clean_df():
    start_time = time.time()
    series = series.map(clean_df)
    print("Cleaning time: ---%s seconds---" % (time.time() - start_time))
    
    # Applying pos_tag:
    start_time = time.time()
    series = series.apply(pos_tag)
    print("Pos tagging time: ---%s seconds---" % (time.time() - start_time))
    
    def pos_tagger(tag_lst):
        
        """ This function takes as input the list of (word,tag) tuples generated by the pos_tag module, 
        translates the tags into wordnet objects and returns a new list of tuples with wordnet tags to be fed
        into the lemmatizer."""
        wordnet_lst = []
        for word, nltk_tag in tag_lst:
            if nltk_tag.startswith('J'):
                wordnet_lst.append((word, wordnet.ADJ))
            elif nltk_tag.startswith('V'):
                wordnet_lst.append((word, wordnet.VERB))
            elif nltk_tag.startswith('N'):
                wordnet_lst.append((word, wordnet.NOUN))
            elif nltk_tag.startswith('R'):
                wordnet_lst.append((word, wordnet.ADV))
            else:          
                wordnet_lst.append((word, None))
        return wordnet_lst
    
    # Applying pos_tagger():
    start_time = time.time()
    series = series.map(pos_tagger)
    print("Translating tags for lemmatizer: ---%s seconds---" % (time.time() - start_time))
    
    # Lemmatizing:
    def lemmata(doc):
        
        """ This function takes as input a list of (word, tag) tuples and returns the lemmatize function
        with pos tags. None tags are ignored"""
        
        lem = []
        for word, tag in doc:
            if tag is None:
                lem.append(word)
            else:
                lem.append(lemmatizer.lemmatize(word, tag))
        return " ".join(lem)
        
    start_time = time.time()
    
    series = series.map(lemmata)
    print("Lemmatizing time: ---%s seconds---" % (time.time() - start_time))
    print("Total preprocessing time: ---%s seconds---" % (time.time() - start))
    
    return series
